{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XHpfv",
      "launcher_item_id": "Zh0CU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Python_Basics_With_Numpy_v3a.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2lpjAivuqvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ktg1ftZqqkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute sigmoid of x.\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    s = 1/(1+math.exp(-x))\n",
        "    \n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTXwqY6zqqkp",
        "colab_type": "text"
      },
      "source": [
        "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
        "    x_1  \\\\\n",
        "    x_2  \\\\\n",
        "    ...  \\\\\n",
        "    x_n  \\\\\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
        "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
        "    ...  \\\\\n",
        "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
        "\\end{pmatrix}\\tag{1} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pdf6cbDqqkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "    s = 1/(1+np.exp(-x))\n",
        "\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3YQpnXcqqk0",
        "colab_type": "text"
      },
      "source": [
        "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcgjbXPGqqk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
        "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
        "    \n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array\n",
        "\n",
        "    Return:\n",
        "    ds -- Your computed gradient.\n",
        "    \"\"\"\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    ds = s*(1-s)\n",
        "    \n",
        "    return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnZiSwIzqqk-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "An image is represented by a 3D array of shape $(length, height, depth = 3)$. Convert it to a vector of shape $(length*height*3, 1)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpSNLX3cqqlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image2vector(image):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    image -- a numpy array of shape (length, height, depth)\n",
        "    \n",
        "    Returns:\n",
        "    v -- a vector of shape (length*height*depth, 1)\n",
        "    \"\"\"\n",
        "    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
        "\n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWSiQPL8qqlN",
        "colab_type": "text"
      },
      "source": [
        "If $$x = \n",
        "\\begin{bmatrix}\n",
        "    0 & 3 & 4 \\\\\n",
        "    2 & 6 & 4 \\\\\n",
        "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
        "    5 \\\\\n",
        "    \\sqrt{56} \\\\\n",
        "\\end{bmatrix}\\tag{4} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
        "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
        "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
        "\\end{bmatrix}\\tag{5}$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ei9LuiJqqlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: normalizeRows\n",
        "\n",
        "def normalizeRows(x):\n",
        "    \"\"\"\n",
        "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
        "    \n",
        "    Argument:\n",
        "    x -- A numpy matrix of shape (n, m)\n",
        "    \n",
        "    Returns:\n",
        "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
        "    \"\"\"\n",
        "    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n",
        "    x = x/x_norm\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoaBmb1vqqlb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
        "    x_1  &&\n",
        "    x_2 &&\n",
        "    ...  &&\n",
        "    x_n  \n",
        "\\end{bmatrix}) = \\begin{bmatrix}\n",
        "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
        "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
        "    ...  &&\n",
        "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
        "\\end{bmatrix} $ \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l39pFcmDqqlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Calculates the softmax for each row of the input x.\n",
        "\n",
        "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
        "\n",
        "    Argument:\n",
        "    x -- A numpy matrix of shape (m,n)\n",
        "\n",
        "    Returns:\n",
        "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
        "    \"\"\"\n",
        "    x_exp = np.exp(x)\n",
        "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
        "    s = x_exp/x_sum\n",
        "\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RJ_YK-Hqqly",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "L1 loss is defined as:\n",
        "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbFo5lS4qqlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L1(yhat, y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- the value of the L1 loss function defined above\n",
        "    \"\"\"\n",
        "    loss = np.sum(np.abs(yhat-y))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrOZ3Dtvqql8",
        "colab_type": "text"
      },
      "source": [
        "L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHfsgdMVqql9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L2(yhat, y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "    \n",
        "    Returns:\n",
        "    loss -- the value of the L2 loss function defined above\n",
        "    \"\"\"\n",
        "    loss = np.sum(np.dot(yhat-y,yhat-y))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}